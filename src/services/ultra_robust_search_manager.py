#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Ultra Robust Search Manager
Gerenciador de busca ultra-robusto com mÃºltiplas camadas e sistemas secundÃ¡rios
"""

import logging
import time
import asyncio
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from services.production_search_manager import production_search_manager
from services.secondary_search_engines import secondary_search_engines
from services.multi_layer_extractor import multi_layer_extractor
from services.url_filter_manager import url_filter_manager
from services.auto_save_manager import salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class UltraRobustSearchManager:
    """Gerenciador de busca ultra-robusto com sistemas redundantes"""
    
    def __init__(self):
        """Inicializa o gerenciador ultra-robusto"""
        self.search_layers = [
            ('primary_engines', self._search_primary_engines),
            ('secondary_engines', self._search_secondary_engines),
            ('specialized_sources', self._search_specialized_sources),
            ('academic_sources', self._search_academic_sources),
            ('news_sources', self._search_news_sources)
        ]
        
        self.extraction_strategies = [
            ('multi_layer', multi_layer_extractor.extract_with_multiple_strategies),
            ('robust_static', self._extract_with_robust_static),
            ('emergency_extraction', self._emergency_extraction)
        ]
        
        self.quality_requirements = {
            'min_sources': 5,
            'min_total_content': 10000,
            'min_avg_quality': 60.0,
            'min_unique_domains': 3
        }
        
        self.stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'total_extractions': 0,
            'successful_extractions': 0,
            'layer_performance': {},
            'quality_metrics': {}
        }
        
        logger.info("Ultra Robust Search Manager inicializado")
    
    def execute_comprehensive_search(
        self, 
        query: str, 
        context: Dict[str, Any],
        max_results: int = 50,
        require_high_quality: bool = True
    ) -> Dict[str, Any]:
        """Executa busca abrangente com mÃºltiplas camadas"""
        
        self.stats['total_searches'] += 1
        
        logger.info(f"ðŸš€ Iniciando busca ultra-robusta para: {query}")
        
        # Salva inÃ­cio da busca
        salvar_etapa("busca_iniciada", {
            "query": query,
            "context": context,
            "max_results": max_results,
            "timestamp": time.time()
        }, categoria="pesquisa_web")
        
        # Gera queries expandidas
        expanded_queries = self._generate_expanded_queries(query, context)
        
        # Salva queries expandidas
        salvar_etapa("queries_expandidas", {
            "original_query": query,
            "expanded_queries": expanded_queries
        }, categoria="pesquisa_web")
        
        # Executa busca em todas as camadas
        all_search_results = []
        
        for layer_name, layer_func in self.search_layers:
            try:
                logger.info(f"ðŸ” Executando camada: {layer_name}")
                
                layer_start_time = time.time()
                layer_results = layer_func(expanded_queries, max_results // len(self.search_layers))
                layer_time = time.time() - layer_start_time
                
                if layer_results:
                    all_search_results.extend(layer_results)
                    
                    # Salva resultados da camada
                    salvar_etapa(f"busca_{layer_name}", {
                        "layer": layer_name,
                        "results_count": len(layer_results),
                        "execution_time": layer_time,
                        "results": layer_results[:10]  # Primeiros 10 para economia de espaÃ§o
                    }, categoria="pesquisa_web")
                    
                    logger.info(f"âœ… {layer_name}: {len(layer_results)} resultados em {layer_time:.2f}s")
                else:
                    logger.warning(f"âš ï¸ {layer_name}: 0 resultados")
                
                # Atualiza estatÃ­sticas da camada
                self.stats['layer_performance'][layer_name] = {
                    'results_count': len(layer_results),
                    'execution_time': layer_time,
                    'success': len(layer_results) > 0
                }
                
            except Exception as e:
                logger.error(f"âŒ Erro na camada {layer_name}: {e}")
                salvar_erro(f"busca_{layer_name}", e, contexto={"query": query})
                continue
        
        # Remove duplicatas e aplica filtros
        filtered_results = self._filter_and_deduplicate(all_search_results)
        
        # Salva resultados filtrados
        salvar_etapa("resultados_filtrados", {
            "original_count": len(all_search_results),
            "filtered_count": len(filtered_results),
            "filter_stats": url_filter_manager.get_stats()
        }, categoria="pesquisa_web")
        
        # Executa extraÃ§Ã£o de conteÃºdo em paralelo
        extracted_content = self._execute_parallel_extraction(filtered_results, context)
        
        # Valida qualidade final
        quality_validation = self._validate_search_quality(extracted_content)
        
        # Salva validaÃ§Ã£o de qualidade
        salvar_etapa("validacao_qualidade_busca", quality_validation, categoria="pesquisa_web")
        
        # Compila resultado final
        final_result = {
            'query': query,
            'expanded_queries': expanded_queries,
            'search_results': filtered_results,
            'extracted_content': extracted_content,
            'quality_validation': quality_validation,
            'statistics': {
                'total_search_results': len(all_search_results),
                'filtered_results': len(filtered_results),
                'successful_extractions': len([c for c in extracted_content if c['success']]),
                'unique_domains': len(set(self._extract_domain(r['url']) for r in filtered_results)),
                'total_content_length': sum(len(c.get('content', '')) for c in extracted_content if c['success']),
                'avg_quality_score': self._calculate_avg_quality(extracted_content)
            },
            'layer_performance': self.stats['layer_performance'],
            'meets_quality_requirements': quality_validation['meets_requirements'],
            'timestamp': time.time()
        }
        
        # Salva resultado final
        salvar_etapa("busca_completa_final", final_result, categoria="pesquisa_web")
        
        if quality_validation['meets_requirements']:
            self.stats['successful_searches'] += 1
            logger.info(f"âœ… Busca ultra-robusta concluÃ­da com sucesso: {len(extracted_content)} extraÃ§Ãµes")
        else:
            logger.warning(f"âš ï¸ Busca concluÃ­da mas nÃ£o atende critÃ©rios de qualidade: {quality_validation['issues']}")
        
        return final_result
    
    def _generate_expanded_queries(self, original_query: str, context: Dict[str, Any]) -> List[str]:
        """Gera queries expandidas para mÃ¡xima cobertura"""
        
        segmento = context.get('segmento', '')
        produto = context.get('produto', '')
        publico = context.get('publico', '')
        
        expanded_queries = [original_query]
        
        # Queries baseadas no contexto
        if segmento:
            expanded_queries.extend([
                f"mercado {segmento} Brasil 2024 dados estatÃ­sticas",
                f"anÃ¡lise competitiva {segmento} oportunidades",
                f"tendÃªncias {segmento} crescimento futuro",
                f"investimento {segmento} venture capital Brasil",
                f"regulamentaÃ§Ã£o {segmento} mudanÃ§as legais",
                f"inovaÃ§Ã£o {segmento} tecnologia disruptiva"
            ])
        
        if produto:
            expanded_queries.extend([
                f"demanda {produto} Brasil consumo",
                f"preÃ§o mÃ©dio {produto} mercado brasileiro",
                f"concorrentes {produto} anÃ¡lise competitiva",
                f"cases sucesso {produto} empresas brasileiras"
            ])
        
        if publico:
            expanded_queries.extend([
                f"comportamento {publico} Brasil pesquisa",
                f"perfil demogrÃ¡fico {publico} dados IBGE",
                f"hÃ¡bitos consumo {publico} tendÃªncias"
            ])
        
        # Queries de inteligÃªncia de mercado
        expanded_queries.extend([
            f"startups {original_query} unicÃ³rnios brasileiros",
            f"fusÃµes aquisiÃ§Ãµes {original_query} M&A Brasil",
            f"IPO {original_query} bolsa valores",
            f"pesquisa mercado {original_query} institutos",
            f"relatÃ³rio setorial {original_query} consultorias"
        ])
        
        # Remove duplicatas e queries muito similares
        unique_queries = []
        for query in expanded_queries:
            if query not in unique_queries and len(query.split()) >= 3:
                unique_queries.append(query)
        
        return unique_queries[:15]  # MÃ¡ximo 15 queries
    
    def _search_primary_engines(self, queries: List[str], max_results: int) -> List[Dict[str, Any]]:
        """Busca nos motores primÃ¡rios"""
        
        primary_results = []
        
        for query in queries[:8]:  # Primeiras 8 queries nos motores primÃ¡rios
            try:
                results = production_search_manager.search_with_fallback(query, max_results // len(queries))
                primary_results.extend(results)
                time.sleep(1)  # Rate limiting
            except Exception as e:
                logger.warning(f"âš ï¸ Erro na busca primÃ¡ria para '{query}': {e}")
                continue
        
        return primary_results
    
    def _search_secondary_engines(self, queries: List[str], max_results: int) -> List[Dict[str, Any]]:
        """Busca nos motores secundÃ¡rios"""
        
        secondary_results = []
        
        for query in queries[8:12]:  # Queries 8-12 nos motores secundÃ¡rios
            try:
                results = secondary_search_engines.search_all_secondary_engines(query, max_results // len(queries))
                secondary_results.extend(results)
                time.sleep(2)  # Rate limiting maior
            except Exception as e:
                logger.warning(f"âš ï¸ Erro na busca secundÃ¡ria para '{query}': {e}")
                continue
        
        return secondary_results
    
    def _search_specialized_sources(self, queries: List[str], max_results: int) -> List[Dict[str, Any]]:
        """Busca em fontes especializadas"""
        
        specialized_results = []
        
        for query in queries[:5]:  # Primeiras 5 queries em fontes especializadas
            try:
                results = secondary_search_engines.search_specialized_databases(query, {}, max_results // len(queries))
                specialized_results.extend(results)
                time.sleep(1.5)
            except Exception as e:
                logger.warning(f"âš ï¸ Erro na busca especializada para '{query}': {e}")
                continue
        
        return specialized_results
    
    def _search_academic_sources(self, queries: List[str], max_results: int) -> List[Dict[str, Any]]:
        """Busca em fontes acadÃªmicas"""
        
        academic_results = []
        
        for query in queries[:3]:  # Primeiras 3 queries em fontes acadÃªmicas
            try:
                results = secondary_search_engines.search_academic_sources(query, max_results // len(queries))
                academic_results.extend(results)
                time.sleep(2)
            except Exception as e:
                logger.warning(f"âš ï¸ Erro na busca acadÃªmica para '{query}': {e}")
                continue
        
        return academic_results
    
    def _search_news_sources(self, queries: List[str], max_results: int) -> List[Dict[str, Any]]:
        """Busca em fontes de notÃ­cias"""
        
        news_results = []
        
        for query in queries[:5]:  # Primeiras 5 queries em notÃ­cias
            try:
                results = secondary_search_engines.search_news_sources(query, max_results // len(queries))
                news_results.extend(results)
                time.sleep(1)
            except Exception as e:
                logger.warning(f"âš ï¸ Erro na busca de notÃ­cias para '{query}': {e}")
                continue
        
        return news_results
    
    def _filter_and_deduplicate(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Filtra e remove duplicatas"""
        
        # Aplica filtros de URL
        filtered_results = url_filter_manager.filtrar_lista_urls(results)
        
        # Remove duplicatas por URL
        seen_urls = set()
        unique_results = []
        
        for result in filtered_results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        # Ordena por prioridade (se disponÃ­vel)
        unique_results.sort(key=lambda x: x.get('filtro', {}).get('prioridade', 1), reverse=True)
        
        return unique_results
    
    def _execute_parallel_extraction(self, search_results: List[Dict[str, Any]], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Executa extraÃ§Ã£o de conteÃºdo em paralelo"""
        
        self.stats['total_extractions'] += len(search_results)
        
        extracted_content = []
        
        # Limita nÃºmero de extraÃ§Ãµes para performance
        results_to_extract = search_results[:30]  # Top 30 resultados
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_result = {
                executor.submit(self._extract_single_url, result, context): result 
                for result in results_to_extract
            }
            
            for future in as_completed(future_to_result):
                result = future_to_result[future]
                try:
                    extraction_result = future.result()
                    extracted_content.append(extraction_result)
                    
                    if extraction_result['success']:
                        self.stats['successful_extractions'] += 1
                        
                except Exception as e:
                    logger.error(f"âŒ Erro na extraÃ§Ã£o paralela de {result.get('url', 'URL desconhecida')}: {e}")
                    extracted_content.append({
                        'success': False,
                        'error': str(e),
                        'url': result.get('url', ''),
                        'title': result.get('title', '')
                    })
        
        return extracted_content
    
    def _extract_single_url(self, search_result: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai conteÃºdo de uma Ãºnica URL usando mÃºltiplas estratÃ©gias"""
        
        url = search_result.get('url', '')
        
        if not url:
            return {
                'success': False,
                'error': 'URL vazia',
                'url': url,
                'title': search_result.get('title', '')
            }
        
        # Tenta cada estratÃ©gia de extraÃ§Ã£o
        for strategy_name, strategy_func in self.extraction_strategies:
            try:
                logger.debug(f"ðŸ” Tentando {strategy_name} para {url}")
                
                if strategy_name == 'multi_layer':
                    result = strategy_func(url, context)
                else:
                    result = strategy_func(url)
                
                if result['success'] and result.get('content'):
                    # Adiciona metadados do resultado de busca
                    result.update({
                        'search_title': search_result.get('title', ''),
                        'search_snippet': search_result.get('snippet', ''),
                        'search_source': search_result.get('source', ''),
                        'extraction_strategy': strategy_name
                    })
                    
                    logger.debug(f"âœ… {strategy_name} bem-sucedida para {url}")
                    return result
                else:
                    logger.debug(f"âš ï¸ {strategy_name} falhou para {url}: {result.get('error', 'Sem conteÃºdo')}")
                    continue
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Erro em {strategy_name} para {url}: {e}")
                continue
        
        # Todas as estratÃ©gias falharam
        return {
            'success': False,
            'error': 'Todas as estratÃ©gias de extraÃ§Ã£o falharam',
            'url': url,
            'title': search_result.get('title', ''),
            'strategies_attempted': len(self.extraction_strategies)
        }
    
    def _extract_with_robust_static(self, url: str) -> Dict[str, Any]:
        """ExtraÃ§Ã£o estÃ¡tica robusta como fallback"""
        
        try:
            from services.robust_content_extractor import robust_content_extractor
            
            content = robust_content_extractor.extract_content(url)
            
            if content:
                return {
                    'success': True,
                    'content': content,
                    'method': 'robust_static_fallback',
                    'url': url
                }
            else:
                return {
                    'success': False,
                    'error': 'ExtraÃ§Ã£o estÃ¡tica nÃ£o retornou conteÃºdo',
                    'content': None
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'Erro na extraÃ§Ã£o estÃ¡tica: {str(e)}',
                'content': None
            }
    
    def _emergency_extraction(self, url: str) -> Dict[str, Any]:
        """ExtraÃ§Ã£o de emergÃªncia como Ãºltimo recurso"""
        
        try:
            import requests
            from bs4 import BeautifulSoup
            
            # Tentativa de emergÃªncia com configuraÃ§Ãµes mÃ­nimas
            response = requests.get(
                url, 
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0 (compatible; ARQV30Bot/2.0)'},
                verify=False
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extrai qualquer texto disponÃ­vel
                for script in soup(["script", "style"]):
                    script.decompose()
                
                text = soup.get_text()
                
                # Filtra linhas com pelo menos 20 caracteres
                lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 20]
                
                if lines:
                    content = '\n'.join(lines[:50])  # Primeiras 50 linhas
                    
                    if len(content) > 100:
                        return {
                            'success': True,
                            'content': content,
                            'method': 'emergency_extraction',
                            'url': url,
                            'warning': 'ExtraÃ§Ã£o de emergÃªncia - qualidade limitada'
                        }
            
            return {
                'success': False,
                'error': 'ExtraÃ§Ã£o de emergÃªncia falhou',
                'content': None
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Erro na extraÃ§Ã£o de emergÃªncia: {str(e)}',
                'content': None
            }
    
    def _validate_search_quality(self, extracted_content: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Valida qualidade da busca completa"""
        
        successful_extractions = [c for c in extracted_content if c['success']]
        
        # Calcula mÃ©tricas
        total_content_length = sum(len(c.get('content', '')) for c in successful_extractions)
        unique_domains = len(set(self._extract_domain(c.get('url', '')) for c in successful_extractions))
        
        # Calcula qualidade mÃ©dia
        quality_scores = []
        for content in successful_extractions:
            if content.get('quality_validation', {}).get('score'):
                quality_scores.append(content['quality_validation']['score'])
        
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # Verifica se atende requisitos
        meets_requirements = (
            len(successful_extractions) >= self.quality_requirements['min_sources'] and
            total_content_length >= self.quality_requirements['min_total_content'] and
            avg_quality >= self.quality_requirements['min_avg_quality'] and
            unique_domains >= self.quality_requirements['min_unique_domains']
        )
        
        # Identifica problemas
        issues = []
        if len(successful_extractions) < self.quality_requirements['min_sources']:
            issues.append(f"Poucas fontes: {len(successful_extractions)} < {self.quality_requirements['min_sources']}")
        
        if total_content_length < self.quality_requirements['min_total_content']:
            issues.append(f"Pouco conteÃºdo: {total_content_length} < {self.quality_requirements['min_total_content']}")
        
        if avg_quality < self.quality_requirements['min_avg_quality']:
            issues.append(f"Qualidade baixa: {avg_quality:.1f} < {self.quality_requirements['min_avg_quality']}")
        
        if unique_domains < self.quality_requirements['min_unique_domains']:
            issues.append(f"Poucos domÃ­nios: {unique_domains} < {self.quality_requirements['min_unique_domains']}")
        
        return {
            'meets_requirements': meets_requirements,
            'successful_extractions': len(successful_extractions),
            'total_content_length': total_content_length,
            'unique_domains': unique_domains,
            'avg_quality_score': avg_quality,
            'issues': issues,
            'quality_distribution': self._analyze_quality_distribution(successful_extractions)
        }
    
    def _extract_domain(self, url: str) -> str:
        """Extrai domÃ­nio de uma URL"""
        
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc.lower().replace('www.', '')
        except:
            return 'unknown'
    
    def _calculate_avg_quality(self, extracted_content: List[Dict[str, Any]]) -> float:
        """Calcula qualidade mÃ©dia"""
        
        quality_scores = []
        
        for content in extracted_content:
            if content['success'] and content.get('quality_validation', {}).get('score'):
                quality_scores.append(content['quality_validation']['score'])
        
        return sum(quality_scores) / len(quality_scores) if quality_scores else 0
    
    def _analyze_quality_distribution(self, successful_extractions: List[Dict[str, Any]]) -> Dict[str, int]:
        """Analisa distribuiÃ§Ã£o de qualidade"""
        
        distribution = {'high': 0, 'medium': 0, 'low': 0}
        
        for content in successful_extractions:
            quality_score = content.get('quality_validation', {}).get('score', 0)
            
            if quality_score >= 80:
                distribution['high'] += 1
            elif quality_score >= 60:
                distribution['medium'] += 1
            else:
                distribution['low'] += 1
        
        return distribution
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """Retorna estatÃ­sticas abrangentes"""
        
        total_searches = self.stats['total_searches']
        search_success_rate = (self.stats['successful_searches'] / total_searches * 100) if total_searches > 0 else 0
        
        total_extractions = self.stats['total_extractions']
        extraction_success_rate = (self.stats['successful_extractions'] / total_extractions * 100) if total_extractions > 0 else 0
        
        return {
            **self.stats,
            'search_success_rate': search_success_rate,
            'extraction_success_rate': extraction_success_rate,
            'primary_engine_status': production_search_manager.get_provider_status(),
            'secondary_engine_status': secondary_search_engines.get_engine_status(),
            'url_filter_stats': url_filter_manager.get_stats()
        }
    
    def reset_all_stats(self):
        """Reset todas as estatÃ­sticas"""
        
        self.stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'total_extractions': 0,
            'successful_extractions': 0,
            'layer_performance': {},
            'quality_metrics': {}
        }
        
        # Reset componentes
        production_search_manager.reset_provider_errors()
        secondary_search_engines.reset_engine_errors()
        url_filter_manager.reset_stats()
        
        logger.info("ðŸ”„ Todas as estatÃ­sticas do sistema de busca resetadas")

# InstÃ¢ncia global
ultra_robust_search_manager = UltraRobustSearchManager()