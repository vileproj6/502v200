#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Enhanced Analysis Pipeline
Pipeline de an√°lise aprimorado sem depend√™ncias do Supabase
"""

import os
import json
import logging
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

class EnhancedAnalysisPipeline:
    """Pipeline de an√°lise aprimorado"""
    
    def __init__(self):
        """Inicializa pipeline"""
        # Importa√ß√µes locais para evitar depend√™ncias circulares
        try:
            from .ai_manager import ai_manager
            self.ai_manager = ai_manager
        except ImportError:
            logger.error("‚ùå AI Manager n√£o dispon√≠vel")
            self.ai_manager = None
        
        try:
            from .production_search_manager import production_search_manager
            self.search_manager = production_search_manager
        except ImportError:
            logger.error("‚ùå Search Manager n√£o dispon√≠vel")
            self.search_manager = None
        
        try:
            from .quality_assurance_manager import quality_assurance_manager
            self.qa_manager = quality_assurance_manager
        except ImportError:
            logger.error("‚ùå QA Manager n√£o dispon√≠vel")
            self.qa_manager = None
        
        try:
            from .auto_save_manager import auto_save_manager, salvar_etapa, salvar_erro
            self.auto_save = auto_save_manager
            self.salvar_etapa = salvar_etapa
            self.salvar_erro = salvar_erro
        except ImportError:
            logger.error("‚ùå Auto Save Manager n√£o dispon√≠vel")
            self.auto_save = None
        
        self.executor = ThreadPoolExecutor(max_workers=os.cpu_count() * 2)
        
        logger.info("‚úÖ Enhanced Analysis Pipeline inicializado")

    def _execute_search_phase(self, query, session_id, progress_callback):
        """Executa fase de pesquisa"""
        try:
            progress_callback(1, "üåê Iniciando pesquisa web e coleta de dados...")
            logger.info(f"Iniciando pesquisa para: {query}")
            
            if not self.search_manager:
                logger.error("‚ùå Search Manager n√£o dispon√≠vel")
                return {"error": "Search Manager n√£o dispon√≠vel"}
            
            search_results = self.search_manager.search_with_fallback(query, max_results=20)
            
            if self.salvar_etapa:
                self.salvar_etapa("pesquisa_web", search_results, categoria="pesquisa_web")
            
            if not search_results:
                logger.warning("Nenhum resultado de pesquisa encontrado.")
                return {"error": "Nenhum resultado de pesquisa encontrado"}
            
            progress_callback(2, "‚úÖ Pesquisa web conclu√≠da.")
            return {"results": search_results, "total": len(search_results)}
            
        except Exception as e:
            logger.error(f"‚ùå Erro na fase de pesquisa: {e}")
            if self.salvar_erro:
                self.salvar_erro("pesquisa_fase", e)
            return {"error": str(e)}

    def _execute_extraction_phase(self, search_results, session_id, progress_callback):
        """Executa fase de extra√ß√£o"""
        try:
            progress_callback(3, "üìÑ Extraindo conte√∫do das p√°ginas...")
            logger.info("Iniciando extra√ß√£o de conte√∫do.")
            
            # Importa extrator
            try:
                from .robust_content_extractor import robust_content_extractor
                extractor = robust_content_extractor
            except ImportError:
                logger.error("‚ùå Content Extractor n√£o dispon√≠vel")
                return {"error": "Content Extractor n√£o dispon√≠vel"}
            
            extracted_content = []
            results = search_results.get("results", [])
            
            for result in results[:15]:  # Limita para performance
                try:
                    content = extractor.extract_content(result.get('url', ''))
                    if content:
                        extracted_content.append({
                            'url': result.get('url'),
                            'title': result.get('title'),
                            'content': content[:2000],  # Limita tamanho
                            'source': result.get('source')
                        })
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao extrair {result.get('url')}: {e}")
                    continue
            
            if self.salvar_etapa:
                self.salvar_etapa("extracao_conteudo", extracted_content, categoria="pesquisa_web")
            
            if not extracted_content:
                logger.warning("Nenhum conte√∫do extra√≠do.")
                return {"error": "Nenhum conte√∫do extra√≠do"}
            
            progress_callback(4, "‚úÖ Extra√ß√£o de conte√∫do conclu√≠da.")
            return extracted_content
            
        except Exception as e:
            logger.error(f"‚ùå Erro na fase de extra√ß√£o: {e}")
            if self.salvar_erro:
                self.salvar_erro("extracao_fase", e)
            return {"error": str(e)}

    def _execute_ai_analysis_phase(self, extracted_content, data, session_id, progress_callback):
        """Executa fase de an√°lise com IA"""
        try:
            progress_callback(5, "üß† Iniciando an√°lise de IA e gera√ß√£o de insights...")
            logger.info("Iniciando an√°lise de IA.")
            
            if not self.ai_manager:
                logger.error("‚ùå AI Manager n√£o dispon√≠vel")
                return {"error": "AI Manager n√£o dispon√≠vel"}
            
            # Prepara contexto para IA
            context = self._prepare_ai_context(extracted_content, data)
            
            # Gera an√°lise principal
            analysis_prompt = self._build_analysis_prompt(data, context)
            
            ai_response = self.ai_manager.generate_analysis(analysis_prompt, max_tokens=8192)
            
            if not ai_response:
                raise Exception("IA n√£o retornou resposta v√°lida")
            
            # Processa resposta
            processed_analysis = self._process_ai_response(ai_response, data)
            
            if self.salvar_etapa:
                self.salvar_etapa("analise_ia", processed_analysis, categoria="analise_completa")
            
            progress_callback(10, "‚úÖ An√°lise de IA conclu√≠da.")
            return processed_analysis
            
        except Exception as e:
            logger.error(f"‚ùå Erro na fase de IA: {e}")
            if self.salvar_erro:
                self.salvar_erro("ia_fase", e)
            return {"error": str(e)}
    
    def _prepare_ai_context(self, extracted_content, data):
        """Prepara contexto para IA"""
        context = "PESQUISA WEB REALIZADA:\n\n"
        
        for i, content_item in enumerate(extracted_content[:10], 1):
            context += f"--- FONTE {i}: {content_item.get('title', 'Sem t√≠tulo')} ---\n"
            context += f"URL: {content_item.get('url', '')}\n"
            context += f"Conte√∫do: {content_item.get('content', '')[:1500]}\n\n"
        
        return context
    
    def _build_analysis_prompt(self, data, context):
        """Constr√≥i prompt para an√°lise"""
        return f"""
Analise o seguinte mercado brasileiro e forne√ßa insights detalhados:

DADOS DO PROJETO:
- Segmento: {data.get('segmento', 'N√£o informado')}
- Produto: {data.get('produto', 'N√£o informado')}
- P√∫blico: {data.get('publico', 'N√£o informado')}
- Pre√ßo: R$ {data.get('preco', 'N√£o informado')}

CONTEXTO DE PESQUISA:
{context}

Forne√ßa uma an√°lise estruturada em JSON com:
1. Avatar ultra-detalhado
2. An√°lise de posicionamento
3. Insights exclusivos (m√≠nimo 15)
4. An√°lise de concorr√™ncia
5. Estrat√©gia de palavras-chave
6. M√©tricas e proje√ß√µes

Retorne apenas JSON v√°lido.
"""
    
    def _process_ai_response(self, ai_response, original_data):
        """Processa resposta da IA"""
        try:
            # Remove markdown se presente
            clean_text = ai_response.strip()
            
            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            elif "```" in clean_text:
                start = clean_text.find("```") + 3
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            
            # Tenta parsear JSON
            analysis = json.loads(clean_text)
            
            # Adiciona metadados
            analysis['metadata'] = {
                'generated_at': datetime.now().isoformat(),
                'provider_used': 'ai_manager',
                'version': '2.0.0',
                'analysis_type': 'comprehensive',
                'data_source': 'real_search_data'
            }
            
            return analysis
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erro ao parsear JSON da IA: {str(e)}")
            # Retorna an√°lise b√°sica
            return self._create_basic_analysis(original_data)
    
    def _create_basic_analysis(self, data):
        """Cria an√°lise b√°sica quando IA falha"""
        segmento = data.get('segmento', 'Neg√≥cios')
        
        return {
            "avatar_ultra_detalhado": {
                "nome_ficticio": f"Profissional {segmento} Brasileiro",
                "perfil_demografico": {
                    "idade": "30-45 anos",
                    "renda": "R$ 8.000 - R$ 35.000",
                    "escolaridade": "Superior completo",
                    "localizacao": "Grandes centros urbanos"
                },
                "dores_viscerais": [
                    f"Trabalhar excessivamente em {segmento} sem crescer",
                    "Sentir-se sempre correndo atr√°s da concorr√™ncia",
                    "Ver competidores crescendo mais r√°pido"
                ],
                "desejos_secretos": [
                    f"Ser autoridade em {segmento}",
                    "Ter liberdade financeira",
                    "Neg√≥cio que funcione sozinho"
                ]
            },
            "insights_exclusivos": [
                f"O mercado brasileiro de {segmento} est√° em transforma√ß√£o",
                "Existe lacuna entre ferramentas e conhecimento",
                f"Profissionais de {segmento} pagam premium por simplicidade",
                "Fator decisivo √© confian√ßa + urg√™ncia + prova social",
                "Sistema b√°sico gerado - configure APIs para an√°lise completa"
            ],
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "analysis_type": "basic_fallback",
                "recommendation": "Configure APIs para an√°lise completa"
            }
        }

    def execute_complete_analysis(self, data, session_id=None, progress_callback=None):
        """Executa an√°lise completa"""
        if progress_callback is None:
            progress_callback = lambda step, message, details=None: logger.info(f"Progresso: {step} - {message}")

        try:
            # Inicia sess√£o de salvamento
            if self.auto_save and not session_id:
                session_id = self.auto_save.iniciar_sessao()
            
            # Fase 1: Pesquisa e Coleta de Dados
            search_results = self._execute_search_phase(data.get("query", f"mercado {data.get('segmento', 'neg√≥cios')} Brasil"), session_id, progress_callback)
            if "error" in search_results:
                logger.warning(f"‚ö†Ô∏è Pesquisa falhou: {search_results['error']}")
                # Continua sem pesquisa
                search_results = {"results": [], "total": 0}

            # Fase 2: Extra√ß√£o de Conte√∫do
            extracted_content = self._execute_extraction_phase(search_results, session_id, progress_callback)
            if isinstance(extracted_content, dict) and "error" in extracted_content:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o falhou: {extracted_content['error']}")
                extracted_content = []

            # Fase 3: An√°lise de IA
            ai_analysis_results = self._execute_ai_analysis_phase(extracted_content, data, session_id, progress_callback)
            if isinstance(ai_analysis_results, dict) and "error" in ai_analysis_results:
                logger.warning(f"‚ö†Ô∏è An√°lise IA falhou: {ai_analysis_results['error']}")
                ai_analysis_results = self._create_basic_analysis(data)

            # Fase 4: Consolida√ß√£o
            progress_callback(11, "üìä Consolidando an√°lise final...")
            
            final_analysis = {
                'projeto_dados': data,
                'pesquisa_web_massiva': {
                    'estatisticas': {
                        'total_resultados': search_results.get('total', 0),
                        'conteudo_extraido': len(extracted_content) if isinstance(extracted_content, list) else 0
                    }
                },
                **ai_analysis_results,
                'session_id': session_id,
                'pipeline_status': {
                    'pesquisa_sucesso': "error" not in search_results,
                    'extracao_sucesso': isinstance(extracted_content, list),
                    'ia_sucesso': isinstance(ai_analysis_results, dict) and "error" not in ai_analysis_results
                }
            }
            
            progress_callback(12, "‚úÖ An√°lise completa finalizada!")
            
            return final_analysis

        except Exception as e:
            logger.error(f"Erro durante a execu√ß√£o completa da an√°lise: {e}", exc_info=True)
            if self.salvar_erro:
                self.salvar_erro("pipeline_completo_falha", e, contexto=data)
            return {"error": f"Erro cr√≠tico na an√°lise: {str(e)}"}

# Inst√¢ncia global
enhanced_analysis_pipeline = EnhancedAnalysisPipeline()