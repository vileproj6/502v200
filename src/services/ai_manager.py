#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - AI Manager com Sistema de Fallback
Gerenciador inteligente de mÃºltiplas IAs com fallback automÃ¡tico
"""

import os
import logging
import time
import json
from typing import Dict, List, Optional, Any
import requests

# Imports condicionais para os clientes de IA
try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False

try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from services.groq_client import groq_client
    HAS_GROQ_CLIENT = True
except ImportError:
    HAS_GROQ_CLIENT = False

logger = logging.getLogger(__name__)

class AIManager:
    """Gerenciador de IAs com sistema de fallback automÃ¡tico"""

    def __init__(self):
        """Inicializa o gerenciador de IAs"""
        self.providers = {
            'gemini': {
                'client': None,
                'available': False,
                'priority': 1,
                'error_count': 0,
                'model': 'gemini-2.0-flash-exp',
                'max_errors': 2,
                'last_success': None,
                'consecutive_failures': 0
            },
            'groq': {
                'client': None,
                'available': False,
                'priority': 2,
                'error_count': 0,
                'model': 'llama-3.3-70b-versatile',
                'max_errors': 2,
                'last_success': None,
                'consecutive_failures': 0
            },
            'openai': {
                'client': None,
                'available': False,
                'priority': 3,
                'error_count': 0,
                'model': 'gpt-3.5-turbo',
                'max_errors': 2,
                'last_success': None,
                'consecutive_failures': 0
            },
            'huggingface': {
                'client': None,
                'available': False,
                'priority': 4,
                'error_count': 0,
                'models': ["HuggingFaceH4/zephyr-7b-beta", "google/flan-t5-base"],
                'current_model_index': 0,
                'max_errors': 3,
                'last_success': None,
                'consecutive_failures': 0
            }
        }

        self.initialize_providers()
        available_count = len([p for p in self.providers.values() if p['available']])
        logger.info(f"ðŸ¤– AI Manager inicializado com {available_count} provedores disponÃ­veis.")

    def initialize_providers(self):
        """Inicializa todos os provedores de IA com base nas chaves de API disponÃ­veis."""

        # Inicializa Gemini
        if HAS_GEMINI:
            try:
                gemini_key = os.getenv('GEMINI_API_KEY')
                if gemini_key:
                    genai.configure(api_key=gemini_key)
                    self.providers['gemini']['client'] = genai.GenerativeModel("gemini-2.0-flash-exp")
                    self.providers['gemini']['available'] = True
                    logger.info("âœ… Gemini (gemini-2.0-flash-exp) inicializado com sucesso")
            except Exception as e:
                logger.warning(f"âš ï¸ Falha ao inicializar Gemini: {str(e)}")
        else:
            logger.warning("âš ï¸ Biblioteca 'google-generativeai' nÃ£o instalada. Gemini desabilitado.")

        # Inicializa OpenAI
        if HAS_OPENAI:
            try:
                openai_key = os.getenv('OPENAI_API_KEY')
                if openai_key:
                    self.providers["openai"]["client"] = openai.OpenAI(api_key=openai_key)
                    self.providers["openai"]["available"] = True
                    logger.info("âœ… OpenAI (gpt-3.5-turbo) inicializado com sucesso")
            except Exception as e:
                logger.warning(f"âš ï¸ Falha ao inicializar OpenAI: {str(e)}")
        else:
            logger.warning("âš ï¸ Biblioteca 'openai' nÃ£o instalada. OpenAI desabilitado.")

        # Inicializa Groq
        try:
            if HAS_GROQ_CLIENT and groq_client and groq_client.is_enabled():
                self.providers['groq']['client'] = groq_client
                self.providers['groq']['available'] = True
                logger.info("âœ… Groq (llama-3.3-70b-versatile) inicializado com sucesso")
            else:
                logger.warning("âš ï¸ Groq client nÃ£o estÃ¡ habilitado")
        except Exception as e:
            logger.warning(f"âš ï¸ Falha ao inicializar Groq: {str(e)}")

        # Inicializa HuggingFace
        try:
            hf_key = os.getenv('HUGGINGFACE_API_KEY')
            if hf_key:
                self.providers['huggingface']['client'] = {
                    'api_key': hf_key,
                    'base_url': 'https://api-inference.huggingface.co/models/'
                }
                self.providers['huggingface']['available'] = True
                logger.info("âœ… HuggingFace inicializado com sucesso")
        except Exception as e:
            logger.warning(f"âš ï¸ Falha ao inicializar HuggingFace: {str(e)}")

    def get_best_provider(self) -> Optional[str]:
        """Retorna o melhor provedor disponÃ­vel com base na prioridade e contagem de erros."""
        current_time = time.time()
        
        # Primeiro, tenta reabilitar provedores que podem ter se recuperado
        for name, provider in self.providers.items():
            if (not provider['available'] and 
                provider.get('last_success') and 
                current_time - provider['last_success'] > 300):  # 5 minutos
                logger.info(f"ðŸ”„ Tentando reabilitar provedor {name} apÃ³s cooldown")
                provider['error_count'] = 0
                provider['consecutive_failures'] = 0
                if name == 'gemini' and HAS_GEMINI:
                    provider['available'] = True
                elif name == 'groq' and HAS_GROQ_CLIENT:
                    provider['available'] = True
                elif name == 'openai' and HAS_OPENAI:
                    provider['available'] = True
                elif name == 'huggingface':
                    provider['available'] = True
        
        available_providers = [
            (name, provider) for name, provider in self.providers.items() 
            if provider['available'] and provider['consecutive_failures'] < provider.get('max_errors', 2)
        ]

        if not available_providers:
            logger.warning("ðŸ”„ Nenhum provedor saudÃ¡vel disponÃ­vel. Resetando contadores.")
            for provider in self.providers.values():
                provider['error_count'] = 0
                provider['consecutive_failures'] = 0
            available_providers = [(name, p) for name, p in self.providers.items() if p['available']]

        if available_providers:
            # Ordena por prioridade e falhas consecutivas
            available_providers.sort(key=lambda x: (x[1]['priority'], x[1]['consecutive_failures']))
            return available_providers[0][0]

        return None

    def generate_analysis(self, prompt: str, max_tokens: int = 8192, provider: Optional[str] = None) -> Optional[str]:
        """Gera anÃ¡lise usando um provedor especÃ­fico ou o melhor disponÃ­vel com fallback."""
        
        start_time = time.time()
        
        # Se um provedor especÃ­fico for solicitado
        if provider:
            if self.providers.get(provider) and self.providers[provider]['available']:
                logger.info(f"ðŸ¤– Usando provedor solicitado: {provider.upper()}")
                try:
                    result = self._call_provider(provider, prompt, max_tokens)
                    if result:
                        self._record_success(provider)
                        return result
                    else:
                        raise Exception("Resposta vazia")
                except Exception as e:
                    logger.error(f"âŒ Provedor solicitado {provider.upper()} falhou: {e}")
                    self._record_failure(provider, str(e))
                    return None # NÃ£o tenta fallback se um provedor especÃ­fico foi pedido e falhou
            else:
                logger.error(f"âŒ Provedor solicitado '{provider}' nÃ£o estÃ¡ disponÃ­vel.")
                return None

        # LÃ³gica de fallback padrÃ£o
        provider_name = self.get_best_provider()
        if not provider_name:
            raise Exception("âŒ NENHUM PROVEDOR DE IA DISPONÃVEL: Configure pelo menos uma API de IA (Gemini, Groq, OpenAI ou HuggingFace)")

        try:
            result = self._call_provider(provider_name, prompt, max_tokens)
            if result:
                self._record_success(provider_name)
                return result
            else:
                raise Exception("Resposta vazia do provedor")
        except Exception as e:
            logger.error(f"âŒ Erro no provedor {provider_name}: {e}")
            self._record_failure(provider_name, str(e))
            return self._try_fallback(prompt, max_tokens, exclude=[provider_name])
    
    def generate_parallel_analysis(self, prompts: List[Dict[str, Any]], max_tokens: int = 8192) -> Dict[str, Any]:
        """Gera mÃºltiplas anÃ¡lises em paralelo usando diferentes provedores"""
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = {}
        
        with ThreadPoolExecutor(max_workers=len(prompts)) as executor:
            future_to_prompt = {}
            
            for prompt_data in prompts:
                prompt_id = prompt_data['id']
                prompt_text = prompt_data['prompt']
                preferred_provider = prompt_data.get('provider')
                
                future = executor.submit(
                    self.generate_analysis, 
                    prompt_text, 
                    max_tokens, 
                    preferred_provider
                )
                future_to_prompt[future] = prompt_id
            
            # Coleta resultados
            for future in as_completed(future_to_prompt, timeout=600):
                prompt_id = future_to_prompt[future]
                try:
                    result = future.result()
                    results[prompt_id] = {
                        'success': bool(result),
                        'content': result,
                        'error': None
                    }
                except Exception as e:
                    results[prompt_id] = {
                        'success': False,
                        'content': None,
                        'error': str(e)
                    }
        
        return results
    
    def _record_success(self, provider_name: str):
        """Registra sucesso do provedor"""
        if provider_name in self.providers:
            self.providers[provider_name]['consecutive_failures'] = 0
            self.providers[provider_name]['last_success'] = time.time()
            logger.info(f"âœ… Sucesso registrado para {provider_name}")
    
    def _record_failure(self, provider_name: str, error_msg: str):
        """Registra falha do provedor"""
        if provider_name in self.providers:
            self.providers[provider_name]['error_count'] += 1
            self.providers[provider_name]['consecutive_failures'] += 1
            
            # Desabilita temporariamente se muitas falhas consecutivas
            if self.providers[provider_name]['consecutive_failures'] >= self.providers[provider_name]['max_errors']:
                logger.warning(f"âš ï¸ Desabilitando {provider_name} temporariamente apÃ³s {self.providers[provider_name]['consecutive_failures']} falhas consecutivas")
                self.providers[provider_name]['available'] = False
            
            logger.error(f"âŒ Falha registrada para {provider_name}: {error_msg}")

    def _call_provider(self, provider_name: str, prompt: str, max_tokens: int) -> Optional[str]:
        """Chama a funÃ§Ã£o de geraÃ§Ã£o do provedor especificado."""
        if provider_name == 'gemini':
            return self._generate_with_gemini(prompt, max_tokens)
        elif provider_name == 'groq':
            return self._generate_with_groq(prompt, max_tokens)
        elif provider_name == 'openai':
            return self._generate_with_openai(prompt, max_tokens)
        elif provider_name == 'huggingface':
            return self._generate_with_huggingface(prompt, max_tokens)
        return None

    def _generate_with_gemini(self, prompt: str, max_tokens: int) -> Optional[str]:
        """Gera conteÃºdo usando Gemini."""
        client = self.providers['gemini']['client']
        config = {
            "temperature": 0.9, 
            "max_output_tokens": min(max_tokens, 8192),
            "top_p": 0.95,
            "top_k": 64
        }
        safety = [
            {"category": c, "threshold": "BLOCK_NONE"} 
            for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]
        ]
        response = client.generate_content(prompt, generation_config=config, safety_settings=safety)
        if response.text:
            logger.info(f"âœ… Gemini gerou {len(response.text)} caracteres")
            return response.text
        raise Exception("Resposta vazia do Gemini")

    def _generate_with_groq(self, prompt: str, max_tokens: int) -> Optional[str]:
        """Gera conteÃºdo usando Groq."""
        client = self.providers['groq']['client']
        content = client.generate(prompt, max_tokens=min(max_tokens, 8192))
        if content:
            logger.info(f"âœ… Groq gerou {len(content)} caracteres")
            return content
        raise Exception("Resposta vazia do Groq")

    def _generate_with_openai(self, prompt: str, max_tokens: int) -> Optional[str]:
        """Gera conteÃºdo usando OpenAI."""
        client = self.providers['openai']['client']
        response = client.chat.completions.create(
            model=self.providers['openai']['model'],
            messages=[
                {"role": "system", "content": "VocÃª Ã© um especialista em anÃ¡lise de mercado ultra-detalhada."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=min(max_tokens, 4096),
            temperature=0.7
        )
        content = response.choices[0].message.content
        if content:
            logger.info(f"âœ… OpenAI gerou {len(content)} caracteres")
            return content
        raise Exception("Resposta vazia do OpenAI")

    def _generate_with_huggingface(self, prompt: str, max_tokens: int) -> Optional[str]:
        """Gera conteÃºdo usando HuggingFace com rotaÃ§Ã£o de modelos."""
        config = self.providers['huggingface']
        for _ in range(len(config['models'])):
            model_index = config['current_model_index']
            model = config['models'][model_index]
            config['current_model_index'] = (model_index + 1) % len(config['models']) # Rotaciona para a prÃ³xima vez
            
            try:
                url = f"{config['client']['base_url']}{model}"
                headers = {"Authorization": f"Bearer {config['client']['api_key']}"}
                payload = {"inputs": prompt, "parameters": {"max_new_tokens": min(max_tokens, 1024)}}
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                
                if response.status_code == 200:
                    res_json = response.json()
                    content = res_json[0].get("generated_text", "")
                    if content:
                        logger.info(f"âœ… HuggingFace ({model}) gerou {len(content)} caracteres")
                        return content
                elif response.status_code == 503:
                    logger.warning(f"âš ï¸ Modelo HuggingFace {model} estÃ¡ carregando (503), tentando prÃ³ximo...")
                    continue
                else:
                    logger.warning(f"âš ï¸ Erro {response.status_code} no modelo {model}")
                    continue
            except Exception as e:
                logger.warning(f"âš ï¸ Erro no modelo {model}: {e}")
                continue
        raise Exception("Todos os modelos HuggingFace falharam")

    def reset_provider_errors(self, provider_name: str = None):
        """Reset contadores de erro dos provedores"""
        if provider_name:
            if provider_name in self.providers:
                self.providers[provider_name]['error_count'] = 0
                self.providers[provider_name]['consecutive_failures'] = 0
                self.providers[provider_name]['available'] = True
                logger.info(f"ðŸ”„ Reset erros do provedor: {provider_name}")
        else:
            for provider in self.providers.values():
                provider['error_count'] = 0
                provider['consecutive_failures'] = 0
                if provider.get('client'):  # SÃ³ reabilita se tem cliente configurado
                    provider['available'] = True
            logger.info("ðŸ”„ Reset erros de todos os provedores")

    def _try_fallback(self, prompt: str, max_tokens: int, exclude: List[str]) -> Optional[str]:
        """Tenta usar o prÃ³ximo provedor disponÃ­vel como fallback."""
        logger.info(f"ðŸ”„ Acionando fallback, excluindo: {', '.join(exclude)}")
        
        # Ordena provedores por prioridade, excluindo os que jÃ¡ falharam
        available_providers = [
            (name, provider) for name, provider in self.providers.items()
            if (provider['available'] and 
                name not in exclude and 
                provider['consecutive_failures'] < provider.get('max_errors', 2))
        ]
        
        if not available_providers:
            logger.critical("âŒ Todos os provedores de fallback falharam.")
            return None
        
        # Ordena por prioridade
        available_providers.sort(key=lambda x: (x[1]['priority'], x[1]['consecutive_failures']))
        next_provider = available_providers[0][0]
        
        logger.info(f"ðŸ”„ Tentando fallback para: {next_provider.upper()}")
        
        try:
            result = self._call_provider(next_provider, prompt, max_tokens)
            if result:
                self._record_success(next_provider)
                return result
            else:
                raise Exception("Resposta vazia do fallback")
        except Exception as e:
            logger.error(f"âŒ Fallback para {next_provider} tambÃ©m falhou: {e}")
            self._record_failure(next_provider, str(e))
            return self._try_fallback(prompt, max_tokens, exclude + [next_provider])
    
    def get_provider_status(self) -> Dict[str, Any]:
        """Retorna status detalhado dos provedores"""
        status = {}
        
        for name, provider in self.providers.items():
            status[name] = {
                'available': provider['available'],
                'priority': provider['priority'],
                'error_count': provider['error_count'],
                'consecutive_failures': provider['consecutive_failures'],
                'last_success': provider.get('last_success'),
                'max_errors': provider['max_errors'],
                'model': provider.get('model', 'N/A')
            }
        
        return status

# InstÃ¢ncia global
ai_manager = AIManager()
