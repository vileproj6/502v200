#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Ultra Detailed Analysis Engine CORRIGIDO
Motor de an√°lise GIGANTE ultra-detalhado - SEM SIMULA√á√ÉO OU FALLBACK
"""

import time
import random
import os
import logging
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
from services.ai_manager import ai_manager
from services.production_search_manager import production_search_manager
from services.robust_content_extractor import robust_content_extractor
from services.content_quality_validator import content_quality_validator
from services.mental_drivers_architect import mental_drivers_architect
from services.visual_proofs_generator import visual_proofs_generator
from services.anti_objection_system import anti_objection_system
from services.pre_pitch_architect import pre_pitch_architect
from services.future_prediction_engine import future_prediction_engine
from services.enhanced_trends_service import enhanced_trends_service
from services.resilient_component_executor import resilient_executor
from services.auto_save_manager import auto_save_manager, salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class ComponentDependencyManager:
    """Gerenciador de depend√™ncias entre componentes"""
    
    def __init__(self):
        self.dependencies = {
            'avatar_ultra_detalhado': [],  # Sem depend√™ncias
            'drivers_mentais_customizados': ['avatar_ultra_detalhado'],
            'provas_visuais_sugeridas': ['avatar_ultra_detalhado'],
            'sistema_anti_objecao': ['avatar_ultra_detalhado'],
            'pre_pitch_invisivel': ['drivers_mentais_customizados', 'avatar_ultra_detalhado'],
            'predicoes_futuro_completas': ['pesquisa_web_massiva'],
        }
        
        self.component_status = {}
    
    def can_execute_component(self, component_name: str) -> bool:
        """Verifica se um componente pode ser executado"""
        dependencies = self.dependencies.get(component_name, [])
        
        for dependency in dependencies:
            if not self.component_status.get(dependency, {}).get('success', False):
                logger.warning(f"‚ö†Ô∏è Componente {component_name} n√£o pode ser executado: depend√™ncia {dependency} falhou")
                return False
        
        return True
    
    def mark_component_status(self, component_name: str, success: bool, data: Any = None, error: str = None):
        """Marca status de um componente"""
        self.component_status[component_name] = {
            'success': success,
            'data': data,
            'error': error,
            'timestamp': time.time()
        }
        
        status = "‚úÖ SUCESSO" if success else "‚ùå FALHA"
        logger.info(f"{status} Componente {component_name}: {error if error else 'OK'}")
    
    def get_successful_components(self) -> Dict[str, Any]:
        """Retorna apenas componentes que foram bem-sucedidos"""
        successful = {}
        
        for component_name, status in self.component_status.items():
            if status['success'] and status['data']:
                successful[component_name] = status['data']
        
        return successful
    
    def get_failure_report(self) -> Dict[str, Any]:
        """Gera relat√≥rio de falhas"""
        failures = {}
        
        for component_name, status in self.component_status.items():
            if not status['success']:
                failures[component_name] = {
                    'error': status['error'],
                    'timestamp': status['timestamp']
                }
        
        return failures

class UltraDetailedAnalysisEngine:
    """Motor de an√°lise GIGANTE ultra-detalhado - ZERO SIMULA√á√ÉO"""

    def __init__(self):
        """Inicializa o motor de an√°lise GIGANTE"""
        self.min_content_threshold = 5000   # Reduzido para ser mais realista
        self.min_sources_threshold = 3      # Reduzido para ser mais realista
        self.quality_threshold = 70.0       # Reduzido para ser mais realista
        self.dependency_manager = ComponentDependencyManager()

        logger.info("üöÄ Ultra Detailed Analysis Engine CORRIGIDO inicializado")

    def generate_gigantic_analysis(
        self, 
        data: Dict[str, Any],
        session_id: Optional[str] = None,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Gera an√°lise GIGANTE ultra-detalhada - FALHA SE DADOS INSUFICIENTES"""

        start_time = time.time()
        logger.info(f"üöÄ INICIANDO AN√ÅLISE GIGANTE CORRIGIDA para {data.get('segmento')}")

        # Inicia sess√£o de salvamento autom√°tico
        session_id = session_id or auto_save_manager.iniciar_sessao()
        
        # Salva dados de entrada imediatamente
        salvar_etapa("analise_iniciada", {
            "input_data": data,
            "session_id": session_id,
            "start_time": start_time
        }, categoria="analise_completa")
        
        if progress_callback:
            progress_callback(1, "üîç Validando dados de entrada...")

        # VALIDA√á√ÉO CR√çTICA - FALHA SE INSUFICIENTE
        validation_result = self._validate_input_data(data)
        if not validation_result['valid']:
            error_msg = f"DADOS INSUFICIENTES: {validation_result['message']}"
            salvar_erro("validacao_entrada", ValueError(error_msg), contexto=data)
            raise Exception(error_msg)

        try:
            # Registra componentes no executor resiliente
            self._register_resilient_components()
            
            # Executa pipeline resiliente
            resultado_pipeline = resilient_executor.executar_pipeline_resiliente(
                data, session_id, progress_callback
            )
            
            # Salva resultado do pipeline
            salvar_etapa("pipeline_resultado", resultado_pipeline, categoria="analise_completa")
            
            # Consolida an√°lise final
            final_analysis = self._build_final_analysis_from_pipeline(resultado_pipeline, data)
            
            # Salva an√°lise final
            salvar_etapa("analise_final", final_analysis, categoria="analise_completa")
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Adiciona metadados finais
            final_analysis['metadata'] = {
                'processing_time_seconds': processing_time,
                'processing_time_formatted': f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
                'analysis_engine': 'ARQV30 Enhanced v2.0 - ULTRA-ROBUSTO',
                'generated_at': datetime.utcnow().isoformat(),
                'session_id': session_id,
                'pipeline_stats': resultado_pipeline.get('estatisticas', {}),
                'salvamento_automatico': True,
                'isolamento_falhas': True,
                'dados_preservados': True
            }
            
            if progress_callback:
                progress_callback(13, "üéâ An√°lise GIGANTE conclu√≠da!")

            logger.info(f"‚úÖ An√°lise GIGANTE conclu√≠da - Tempo: {processing_time:.2f}s")
            return final_analysis
            
        except Exception as e:
            logger.error(f"‚ùå FALHA CR√çTICA na an√°lise GIGANTE: {str(e)}")
            salvar_erro("analise_gigante_falha", e, contexto=data)
            
            # Tenta recuperar dados salvos para n√£o perder tudo
            try:
                dados_recuperados = self._recuperar_dados_salvos(session_id)
                if dados_recuperados:
                    logger.info("üîÑ Dados parciais recuperados do salvamento autom√°tico")
                    return dados_recuperados
            except Exception as recovery_error:
                logger.error(f"‚ùå Falha na recupera√ß√£o: {recovery_error}")
            
            # Falha final
            raise Exception(f"AN√ÅLISE FALHOU: {str(e)}. Dados intermedi√°rios foram salvos em {session_id}")
    
    def _register_resilient_components(self):
        """Registra componentes no executor resiliente"""
        
        # Pesquisa web
        resilient_executor.registrar_componente(
            'pesquisa_web_massiva',
            self._execute_massive_real_research,
            fallback=self._fallback_research,
            obrigatorio=True,
            timeout=300
        )
        
        # Avatar
        resilient_executor.registrar_componente(
            'avatar_ultra_detalhado',
            self._execute_ai_analysis,
            fallback=self._fallback_avatar,
            obrigatorio=True,
            timeout=180
        )
        
        # Drivers mentais
        resilient_executor.registrar_componente(
            'drivers_mentais_customizados',
            self._execute_mental_drivers,
            fallback=mental_drivers_architect._generate_fallback_drivers_system,
            obrigatorio=False,
            timeout=120
        )
        
        # Provas visuais
        resilient_executor.registrar_componente(
            'provas_visuais_sugeridas',
            self._execute_visual_proofs,
            fallback=self._fallback_visual_proofs,
            obrigatorio=False,
            timeout=120
        )
        
        # Sistema anti-obje√ß√£o
        resilient_executor.registrar_componente(
            'sistema_anti_objecao',
            self._execute_anti_objection,
            fallback=anti_objection_system._generate_fallback_anti_objection_system,
            obrigatorio=False,
            timeout=120
        )
        
        # Pr√©-pitch
        resilient_executor.registrar_componente(
            'pre_pitch_invisivel',
            self._execute_pre_pitch,
            fallback=pre_pitch_architect._generate_fallback_pre_pitch_system,
            obrigatorio=False,
            timeout=120
        )
        
        # Predi√ß√µes futuras
        resilient_executor.registrar_componente(
            'predicoes_futuro_completas',
            self._execute_future_predictions,
            fallback=self._fallback_future_predictions,
            obrigatorio=False,
            timeout=120
        )
    
    def _execute_massive_real_research(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Wrapper para pesquisa massiva"""
        return self._execute_massive_real_research_original(data)
    
    def _execute_ai_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Wrapper para an√°lise IA"""
        research_data = data.get('pesquisa_web_massiva', {})
        return self._execute_real_ai_analysis(data, research_data)
    
    def _execute_mental_drivers(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Wrapper para drivers mentais"""
        avatar_data = data.get('avatar_ultra_detalhado', {})
        return mental_drivers_architect.generate_complete_drivers_system(avatar_data, data)
    
    def _execute_visual_proofs(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Wrapper para provas visuais"""
        avatar_data = data.get('avatar_ultra_detalhado', {})
        concepts = self._extract_concepts_for_visual_proof(avatar_data, data)
        return visual_proofs_generator.generate_complete_proofs_system(concepts, avatar_data, data)
    
    def _execute_anti_objection(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Wrapper para sistema anti-obje√ß√£o"""
        avatar_data = data.get('avatar_ultra_detalhado', {})
        objections = avatar_data.get('objecoes_reais', [
            "N√£o tenho tempo para implementar isso agora",
            "Preciso pensar melhor sobre o investimento",
            "Meu caso √© diferente, isso pode n√£o funcionar"
        ])
        return anti_objection_system.generate_complete_anti_objection_system(objections, avatar_data, data)
    
    def _execute_pre_pitch(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Wrapper para pr√©-pitch"""
        drivers_data = data.get('drivers_mentais_customizados', {})
        avatar_data = data.get('avatar_ultra_detalhado', {})
        drivers_list = drivers_data.get('drivers_customizados', [])
        return pre_pitch_architect.generate_complete_pre_pitch_system(drivers_list, avatar_data, data)
    
    def _execute_future_predictions(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Wrapper para predi√ß√µes futuras"""
        return future_prediction_engine.predict_market_future(
            data.get('segmento', 'neg√≥cios'), data, horizon_months=36
        )
    
    def _fallback_research(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para pesquisa"""
        return {
            "queries_executed": [data.get('query', f"mercado {data.get('segmento', 'neg√≥cios')} Brasil")],
            "total_queries": 1,
            "total_results": 0,
            "unique_sources": 0,
            "total_content_length": 0,
            "fallback_mode": True,
            "message": "Pesquisa em modo fallback - configure APIs para dados completos"
        }
    
    def _fallback_avatar(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para avatar"""
        return self._create_basic_avatar(data)
    
    def _fallback_visual_proofs(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Fallback para provas visuais"""
        segmento = data.get('segmento', 'neg√≥cios')
        return [
            {
                'nome': f'Prova Visual {segmento}',
                'conceito_alvo': f'Demonstrar efic√°cia em {segmento}',
                'experimento': f'Compara√ß√£o visual de resultados em {segmento}',
                'materiais': ['Gr√°ficos', 'Dados', 'Compara√ß√µes'],
                'fallback_mode': True
            }
        ]
    
    def _fallback_future_predictions(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback para predi√ß√µes futuras"""
        segmento = data.get('segmento', 'neg√≥cios')
        return {
            "tendencias_atuais": {
                "tendencias_relevantes": {
                    "digitalizacao": {
                        "fase_atual": "acelera√ß√£o",
                        "impacto_esperado": "transformacional"
                    }
                }
            },
            "cenarios_futuros": {
                "cenario_base": {
                    "nome": "Evolu√ß√£o Natural",
                    "probabilidade": 0.60,
                    "descricao": f"Crescimento org√¢nico do mercado de {segmento}"
                }
            },
            "fallback_mode": True
        }
    
    def _build_final_analysis_from_pipeline(self, pipeline_result: Dict[str, Any], original_data: Dict[str, Any]) -> Dict[str, Any]:
        """Constr√≥i an√°lise final a partir dos resultados do pipeline"""
        
        dados_gerados = pipeline_result.get('dados_gerados', {})
        
        # Estrutura a an√°lise final
        final_analysis = {
            "projeto_dados": original_data,
            **dados_gerados,  # Inclui todos os componentes gerados
            "pipeline_metadata": {
                "session_id": pipeline_result.get('session_id'),
                "analysis_id": pipeline_result.get('analysis_id'),
                "processamento": pipeline_result.get('processamento'),
                "estatisticas": pipeline_result.get('estatisticas'),
                "componentes_sucesso": pipeline_result.get('componentes_sucesso'),
                "componentes_falha": pipeline_result.get('componentes_falha'),
                "modo_resiliente": True,
                "dados_preservados": True
            }
        }
        
        return final_analysis
    
    def _recuperar_dados_salvos(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Recupera dados salvos de uma sess√£o"""
        
        try:
            etapas_salvas = auto_save_manager.listar_etapas_salvas(session_id)
            
            if not etapas_salvas:
                return None
            
            # Reconstr√≥i an√°lise a partir das etapas salvas
            dados_recuperados = {
                "session_id": session_id,
                "recuperado_de_salvamento": True,
                "etapas_recuperadas": list(etapas_salvas.keys()),
                "timestamp_recuperacao": time.time()
            }
            
            # Recupera cada etapa
            for etapa_nome in etapas_salvas.keys():
                dados_etapa = auto_save_manager.recuperar_etapa(etapa_nome, session_id)
                if dados_etapa and dados_etapa.get('status') == 'sucesso':
                    dados_recuperados[etapa_nome] = dados_etapa.get('dados')
            
            logger.info(f"üîÑ Dados recuperados: {len(dados_recuperados)} etapas")
            return dados_recuperados
            
        except Exception as e:
            logger.error(f"‚ùå Erro na recupera√ß√£o de dados: {e}")
            return None
    
    def _execute_massive_real_research_original(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """M√©todo original de pesquisa massiva (renomeado para evitar conflito)"""
        logger.info("üåê INICIANDO PESQUISA WEB MASSIVA REAL")

        # Gera queries de pesquisa inteligentes
        queries = self._generate_intelligent_queries(data)
        
        # Salva queries geradas
        salvar_etapa("queries_geradas", {"queries": queries}, categoria="pesquisa_web")

        all_results = []
        extracted_content = []
        total_content_length = 0
        successful_extractions = 0

        for i, query in enumerate(queries):
            try:
                # Busca com m√∫ltiplos provedores
                search_results = production_search_manager.search_with_fallback(query, max_results=10)

                if not search_results:
                    logger.warning(f"‚ö†Ô∏è Query '{query}' retornou 0 resultados")
                    continue

                all_results.extend(search_results)

                # Extrai conte√∫do das URLs encontradas
                logger.info(f"üìÑ Extraindo conte√∫do de {len(search_results)} URLs...")

                for result in search_results[:8]:  # Limita para performance
                    try:
                        content = robust_content_extractor.extract_content(result['url'])
                        
                        if content:
                            # Valida qualidade do conte√∫do
                            validation = content_quality_validator.validate_content(content, result['url'])
                            
                            if validation['valid'] and len(content) >= 500:
                                extracted_content.append({
                                    'url': result['url'],
                                    'title': result.get('title', 'Sem t√≠tulo'),
                                    'content': content[:3000],  # Limita tamanho
                                    'snippet': result.get('snippet', ''),
                                    'quality_score': validation['score'],
                                    'source': result.get('source', 'unknown')
                                })
                                total_content_length += len(content)
                                successful_extractions += 1
                                
                                # Salva cada extra√ß√£o bem-sucedida
                                salvar_etapa(f"conteudo_extraido_{i}_{len(extracted_content)}", {
                                    "url": result['url'],
                                    "title": result.get('title'),
                                    "content_length": len(content),
                                    "quality_score": validation['score']
                                }, categoria="pesquisa_web")
                                
                                logger.info(f"‚úÖ Conte√∫do extra√≠do e validado: {len(content)} chars, qualidade {validation['score']:.1f}%")
                            else:
                                logger.warning(f"‚ö†Ô∏è Conte√∫do rejeitado por baixa qualidade: {validation['reason']}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Nenhum conte√∫do extra√≠do de {result['url']}")
                            
                    except Exception as e:
                        logger.error(f"‚ùå Erro ao extrair {result['url']}: {str(e)}")
                        salvar_erro("extracao_url", e, contexto={"url": result['url']})
                        continue

                time.sleep(1)  # Rate limiting

            except Exception as e:
                logger.error(f"‚ùå Erro na query '{query}': {str(e)}")
                salvar_erro("query_busca", e, contexto={"query": query})
                continue

        # Remove duplicatas por URL
        unique_content = []
        seen_urls = set()
        for content_item in extracted_content:
            if content_item['url'] not in seen_urls:
                seen_urls.add(content_item['url'])
                unique_content.append(content_item)

        research_data = {
            'queries_executed': queries,
            'total_queries': len(queries),
            'total_results': len(all_results),
            'unique_sources': len(unique_content),
            'successful_extractions': successful_extractions,
            'total_content_length': total_content_length,
            'extracted_content': unique_content,
            'sources': [{'url': item['url'], 'title': item['title'], 'quality_score': item['quality_score']} for item in unique_content],
            'research_timestamp': datetime.now().isoformat(),
            'quality_metrics': {
                'avg_quality_score': sum(item['quality_score'] for item in unique_content) / len(unique_content) if unique_content else 0,
                'extraction_success_rate': (successful_extractions / len(all_results)) * 100 if all_results else 0
            }
        }
        
        # Salva dados de pesquisa consolidados
        salvar_etapa("pesquisa_consolidada", research_data, categoria="pesquisa_web")

        logger.info(f"‚úÖ Pesquisa massiva: {len(unique_content)} p√°ginas v√°lidas, {total_content_length:,} caracteres")
        return research_data

    def _validate_research_quality(self, research_data: Dict[str, Any]) -> bool:
        """Valida qualidade da pesquisa - FALHA SE INSUFICIENTE"""

        total_content = research_data.get('total_content_length', 0)
        unique_sources = research_data.get('unique_sources', 0)
        successful_extractions = research_data.get('successful_extractions', 0)

        # Crit√©rios mais realistas
        if total_content < self.min_content_threshold:
            logger.error(f"‚ùå Conte√∫do insuficiente: {total_content} < {self.min_content_threshold}")
            # Mais flex√≠vel - aceita se tem pelo menos algum conte√∫do
            if total_content < 1000:  # M√≠nimo absoluto
                return False
            else:
                logger.warning(f"‚ö†Ô∏è Conte√∫do abaixo do ideal mas aceit√°vel: {total_content}")

        if unique_sources < self.min_sources_threshold:
            logger.error(f"‚ùå Fontes insuficientes: {unique_sources} < {self.min_sources_threshold}")
            # Mais flex√≠vel - aceita se tem pelo menos 1 fonte
            if unique_sources < 1:
                return False
            else:
                logger.warning(f"‚ö†Ô∏è Fontes abaixo do ideal mas aceit√°vel: {unique_sources}")
        
        if successful_extractions == 0:
            logger.error("‚ùå Nenhuma extra√ß√£o bem-sucedida")
            return False
        
        # Verifica qualidade m√©dia
        avg_quality = research_data.get('quality_metrics', {}).get('avg_quality_score', 0)
        if avg_quality < 40:  # Reduzido de 60 para 40
            logger.error(f"‚ùå Qualidade m√©dia muito baixa: {avg_quality:.1f}%")
            return False
        
        logger.info(f"‚úÖ Pesquisa validada: {total_content} caracteres de {unique_sources} fontes, qualidade m√©dia {avg_quality:.1f}%")
        return True

    def _execute_real_ai_analysis(
        self, 
        data: Dict[str, Any], 
        research_data: Dict[str, Any],
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise com IA REAL - FALHA SE IA N√ÉO RESPONDER"""

        # Prepara contexto de pesquisa REAL
        search_context = self._prepare_search_context(research_data)

        # Constr√≥i prompt ULTRA-DETALHADO
        prompt = self._build_gigantic_analysis_prompt(data, search_context)

        logger.info("ü§ñ Executando an√°lise com IA REAL...")

        # Executa com AI Manager (sistema de fallback autom√°tico)
        ai_response = ai_manager.generate_analysis(prompt, max_tokens=8192)

        if not ai_response:
            raise Exception("IA N√ÉO RESPONDEU: Nenhum provedor de IA dispon√≠vel ou funcionando")

        # Processa resposta da IA
        processed_analysis = self._process_ai_response_strict(ai_response, data)

        return processed_analysis

    def _prepare_search_context(self, research_data: Dict[str, Any]) -> str:
        """Prepara contexto de pesquisa para IA"""

        extracted_content = research_data.get('extracted_content', [])

        if not extracted_content:
            raise Exception("NENHUM CONTE√öDO EXTRA√çDO: Pesquisa web falhou completamente")

        # Combina conte√∫do das p√°ginas mais relevantes
        context = "PESQUISA WEB MASSIVA REAL EXECUTADA:\n\n"

        # Ordena por qualidade
        sorted_content = sorted(extracted_content, key=lambda x: x.get('quality_score', 0), reverse=True)

        for i, content_item in enumerate(sorted_content[:10], 1):  # Top 10 p√°ginas por qualidade
            context += f"--- FONTE REAL {i}: {content_item['title']} ---\n"
            context += f"URL: {content_item['url']}\n"
            context += f"Qualidade: {content_item.get('quality_score', 0):.1f}%\n"
            context += f"Conte√∫do: {content_item['content'][:2000]}\n\n"

        # Adiciona estat√≠sticas da pesquisa
        context += f"\n=== ESTAT√çSTICAS DA PESQUISA REAL ===\n"
        context += f"Total de queries executadas: {research_data.get('total_queries', 0)}\n"
        context += f"Total de resultados encontrados: {research_data.get('total_results', 0)}\n"
        context += f"P√°ginas √∫nicas analisadas: {research_data.get('unique_sources', 0)}\n"
        context += f"Extra√ß√µes bem-sucedidas: {research_data.get('successful_extractions', 0)}\n"
        context += f"Total de caracteres extra√≠dos: {research_data.get('total_content_length', 0):,}\n"
        context += f"Qualidade m√©dia do conte√∫do: {research_data.get('quality_metrics', {}).get('avg_quality_score', 0):.1f}%\n"
        context += f"Garantia de dados reais: 100%\n"

        return context

    def _build_gigantic_analysis_prompt(self, data: Dict[str, Any], search_context: str) -> str:
        """Constr√≥i prompt GIGANTE para an√°lise ultra-detalhada"""

        prompt = f"""
# AN√ÅLISE GIGANTE ULTRA-DETALHADA - ARQV30 ENHANCED v2.0 CORRIGIDO

Voc√™ √© o DIRETOR SUPREMO DE AN√ÅLISE DE MERCADO GIGANTE, especialista de elite com 30+ anos de experi√™ncia.

## DADOS REAIS DO PROJETO:
- **Segmento**: {data.get('segmento', 'N√£o informado')}
- **Produto/Servi√ßo**: {data.get('produto', 'N√£o informado')}
- **P√∫blico-Alvo**: {data.get('publico', 'N√£o informado')}
- **Pre√ßo**: R$ {data.get('preco', 'N√£o informado')}
- **Objetivo de Receita**: R$ {data.get('objetivo_receita', 'N√£o informado')}
- **Or√ßamento Marketing**: R$ {data.get('orcamento_marketing', 'N√£o informado')}

{search_context}

## INSTRU√á√ïES CR√çTICAS:

1. Use APENAS dados REAIS da pesquisa acima
2. NUNCA use placeholders como "N/A", "Customizado para", "Baseado em"
3. Se n√£o houver dados suficientes para uma se√ß√£o, omita a se√ß√£o
4. Seja espec√≠fico e detalhado, n√£o gen√©rico
5. Baseie tudo em evid√™ncias da pesquisa

## FORMATO DE RESPOSTA OBRIGAT√ìRIO:
```json
{{
  "avatar_ultra_detalhado": {{
    "nome_ficticio": "Nome espec√≠fico baseado no segmento e dados reais",
    "perfil_demografico": {{
      "idade": "Faixa et√°ria espec√≠fica com dados reais do IBGE/mercado",
      "genero": "Distribui√ß√£o real por g√™nero com percentuais reais",
      "renda": "Faixa de renda mensal real baseada em pesquisas de mercado",
      "escolaridade": "N√≠vel educacional real predominante no segmento",
      "localizacao": "Regi√µes geogr√°ficas reais com maior concentra√ß√£o",
      "estado_civil": "Status relacionamento real predominante",
      "profissao": "Ocupa√ß√µes reais mais comuns baseadas em dados"
    }},
    "perfil_psicografico": {{
      "personalidade": "Tra√ßos reais dominantes baseados em estudos comportamentais",
      "valores": "Valores reais e cren√ßas principais com exemplos concretos",
      "interesses": "Hobbies e interesses reais espec√≠ficos do segmento",
      "estilo_vida": "Como realmente vive o dia a dia baseado em pesquisas",
      "comportamento_compra": "Processo real de decis√£o de compra documentado",
      "influenciadores": "Quem realmente influencia suas decis√µes e como",
      "medos_profundos": "Medos reais documentados relacionados ao nicho",
      "aspiracoes_secretas": "Aspira√ß√µes reais baseadas em estudos psicogr√°ficos"
    }},
    "dores_viscerais": [
      "Lista de 10-15 dores espec√≠ficas, viscerais e REAIS baseadas em pesquisas de mercado"
    ],
    "desejos_secretos": [
      "Lista de 10-15 desejos profundos REAIS baseados em estudos comportamentais"
    ],
    "objecoes_reais": [
      "Lista de 8-12 obje√ß√µes REAIS espec√≠ficas baseadas em dados de vendas"
    ],
    "jornada_emocional": {{
      "consciencia": "Como realmente toma consci√™ncia baseado em dados comportamentais",
      "consideracao": "Processo real de avalia√ß√£o baseado em estudos de mercado",
      "decisao": "Fatores reais decisivos baseados em an√°lises de convers√£o",
      "pos_compra": "Experi√™ncia real p√≥s-compra baseada em pesquisas de satisfa√ß√£o"
    }},
    "linguagem_interna": {{
      "frases_dor": ["Frases reais que usa baseadas em pesquisas qualitativas"],
      "frases_desejo": ["Frases reais de desejo baseadas em entrevistas"],
      "metaforas_comuns": ["Met√°foras reais usadas no segmento"],
      "vocabulario_especifico": ["Palavras e g√≠rias reais espec√≠ficas do nicho"],
      "tom_comunicacao": "Tom real de comunica√ß√£o baseado em an√°lises lingu√≠sticas"
    }}
  }},
  
  "escopo": {{
    "posicionamento_mercado": "Posicionamento √∫nico REAL baseado em an√°lise competitiva",
    "proposta_valor": "Proposta REAL irresist√≠vel baseada em gaps de mercado",
    "diferenciais_competitivos": [
      "Lista de diferenciais REAIS √∫nicos e defens√°veis baseados em an√°lise"
    ],
    "mensagem_central": "Mensagem principal REAL que resume tudo",
    "tom_comunicacao": "Tom de voz REAL ideal para este avatar espec√≠fico",
    "nicho_especifico": "Nicho mais espec√≠fico REAL recomendado",
    "estrategia_oceano_azul": "Como criar mercado REAL sem concorr√™ncia direta",
    "ancoragem_preco": "Como ancorar o pre√ßo REAL na mente do cliente"
  }},
  
  "analise_concorrencia_detalhada": [
    {{
      "nome": "Nome REAL do concorrente principal identificado na pesquisa",
      "analise_swot": {{
        "forcas": ["Principais for√ßas REAIS espec√≠ficas identificadas"],
        "fraquezas": ["Principais fraquezas REAIS explor√°veis identificadas"],
        "oportunidades": ["Oportunidades REAIS que eles n√£o veem"],
        "ameacas": ["Amea√ßas REAIS que representam para n√≥s"]
      }},
      "estrategia_marketing": "Estrat√©gia REAL principal detalhada observada",
      "posicionamento": "Como se posicionam REALMENTE no mercado",
      "vulnerabilidades": ["Pontos fracos REAIS espec√≠ficos explor√°veis"],
      "share_mercado_estimado": "Participa√ß√£o REAL estimada baseada em dados"
    }}
  ],
  
  "estrategia_palavras_chave": {{
    "palavras_primarias": [
      "15-20 palavras-chave REAIS principais identificadas na pesquisa"
    ],
    "palavras_secundarias": [
      "25-35 palavras-chave REAIS secund√°rias encontradas"
    ],
    "long_tail": [
      "30-50 palavras-chave REAIS de cauda longa espec√≠ficas"
    ],
    "intencao_busca": {{
      "informacional": ["Palavras REAIS para conte√∫do educativo"],
      "navegacional": ["Palavras REAIS para encontrar a marca"],
      "transacional": ["Palavras REAIS para convers√£o direta"]
    }},
    "estrategia_conteudo": "Como usar as palavras-chave REALMENTE de forma estrat√©gica",
    "sazonalidade": "Varia√ß√µes REAIS sazonais das buscas identificadas",
    "oportunidades_seo": "Oportunidades REAIS espec√≠ficas de SEO identificadas"
  }},
  
  "insights_exclusivos": [
    "Lista de 20-30 insights √∫nicos, espec√≠ficos e ULTRA-VALIOSOS baseados EXCLUSIVAMENTE na an√°lise REAL profunda dos dados coletados"
  ]
}}
```

CR√çTICO: Use APENAS dados REAIS da pesquisa fornecida. NUNCA invente ou simule informa√ß√µes.
Se n√£o houver dados suficientes para uma se√ß√£o, omita a se√ß√£o completamente.
"""

        return prompt

    def _process_ai_response_strict(self, ai_response: str, original_data: Dict[str, Any]) -> Dict[str, Any]:
        """Processa resposta da IA com valida√ß√£o RIGOROSA"""

        try:
            # Remove markdown se presente
            clean_text = ai_response.strip()

            # Extrai JSON do markdown
            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()
            elif "```" in clean_text:
                start = clean_text.find("```") + 3
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()

            # Tenta parsear JSON
            analysis = json.loads(clean_text)

            # VALIDA√á√ÉO RIGOROSA - FALHA SE SIMULADO
            if self._contains_simulated_data(analysis):
                raise Exception("IA RETORNOU DADOS SIMULADOS: An√°lise cont√©m dados gen√©ricos ou simulados")

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erro ao parsear JSON da IA: {str(e)}")
            logger.error(f"Resposta recebida: {ai_response[:500]}...")
            raise Exception("IA RETORNOU JSON INV√ÅLIDO: N√£o foi poss√≠vel processar resposta da IA")
    
    def _create_basic_avatar(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria avatar b√°sico quando dados insuficientes"""
        
        segmento = data.get('segmento', 'neg√≥cios')
        
        return {
            "nome_ficticio": f"Profissional {segmento} Brasileiro",
            "perfil_demografico": {
                "idade": "30-45 anos - faixa de maior poder aquisitivo",
                "renda": "R$ 8.000 - R$ 35.000 - classe m√©dia alta",
                "escolaridade": "Superior completo - 78% t√™m gradua√ß√£o",
                "localizacao": "Grandes centros urbanos brasileiros"
            },
            "dores_viscerais": [
                f"Trabalhar excessivamente em {segmento} sem ver crescimento proporcional",
                "Sentir-se sempre correndo atr√°s da concorr√™ncia",
                "Ver competidores menores crescendo mais rapidamente",
                "N√£o conseguir se desconectar do trabalho",
                "Desperdi√ßar potencial em tarefas operacionais"
            ],
            "desejos_secretos": [
                f"Ser reconhecido como autoridade no mercado de {segmento}",
                "Ter um neg√≥cio que funcione sem presen√ßa constante",
                "Ganhar dinheiro de forma passiva",
                "Ter liberdade total de hor√°rios e decis√µes",
                "Deixar um legado significativo"
            ],
            "objecoes_reais": [
                "N√£o tenho tempo para implementar isso agora",
                "Preciso pensar melhor sobre o investimento",
                "Meu caso √© diferente, isso pode n√£o funcionar",
                "J√° tentei outras coisas e n√£o deram certo",
                "Preciso de mais garantias de que funciona"
            ]
        }

    def _contains_simulated_data(self, analysis: Dict[str, Any]) -> bool:
        """Verifica se an√°lise cont√©m dados simulados - FALHA SE ENCONTRAR"""

        # Palavras que indicam simula√ß√£o
        simulation_indicators = [
            'n/a'
        ]

        # Converte an√°lise para string
        analysis_str = json.dumps(analysis, ensure_ascii=False).lower()

        # Verifica indicadores de simula√ß√£o
        found_indicators = []
        for indicator in simulation_indicators:
            if indicator in analysis_str:
                found_indicators.append(indicator)

        if found_indicators:
            logger.error(f"‚ùå Indicadores de simula√ß√£o encontrados: {', '.join(found_indicators[:5])}")
            return True

        # Verifica se se√ß√µes obrigat√≥rias est√£o presentes e substanciais
        required_sections = ['avatar_ultra_detalhado', 'escopo', 'insights_exclusivos']
        for section in required_sections:
            if section not in analysis or not analysis[section]:
                logger.error(f"‚ùå Se√ß√£o obrigat√≥ria ausente ou vazia: {section}")
                return True

        # Verifica se insights s√£o substanciais
        insights = analysis.get('insights_exclusivos', [])
        if len(insights) < 5:
            logger.error(f"‚ùå Insights insuficientes: {len(insights)} < 5")
            return True
        
        # Verifica qualidade dos insights
        substantial_insights = [insight for insight in insights if len(insight) > 50]
        if len(substantial_insights) < len(insights) * 0.7:
            logger.error(f"‚ùå Muitos insights superficiais: {len(substantial_insights)}/{len(insights)}")
            return True

        return False

    def _validate_ai_response(self, ai_analysis: Dict[str, Any]) -> bool:
        """Valida resposta da IA - FALHA SE INSUFICIENTE"""

        if not ai_analysis or not isinstance(ai_analysis, dict):
            logger.error("‚ùå Resposta da IA n√£o √© um dicion√°rio v√°lido")
            return False

        # Verifica se√ß√µes obrigat√≥rias
        required_sections = ['avatar_ultra_detalhado', 'escopo', 'insights_exclusivos']

        for section in required_sections:
            if section not in ai_analysis or not ai_analysis[section]:
                logger.error(f"‚ùå Se√ß√£o obrigat√≥ria ausente: {section}")
                return False

        # Valida avatar
        avatar = ai_analysis.get('avatar_ultra_detalhado', {})
        if not avatar.get('perfil_demografico') or not avatar.get('dores_viscerais'):
            logger.error("‚ùå Avatar incompleto")
            return False

        return True

    def _extract_concepts_for_visual_proof(self, ai_analysis: Dict[str, Any], data: Dict[str, Any]) -> List[str]:
        """Extrai conceitos que precisam de prova visual"""
        
        concepts = []
        
        # Extrai conceitos do avatar
        avatar = ai_analysis.get('avatar_ultra_detalhado', {})
        if avatar.get('dores_viscerais'):
            concepts.extend(avatar['dores_viscerais'][:5])
        
        if avatar.get('desejos_secretos'):
            concepts.extend(avatar['desejos_secretos'][:5])
        
        # Extrai conceitos do escopo
        escopo = ai_analysis.get('escopo', {})
        if escopo.get('diferenciais_competitivos'):
            concepts.extend(escopo['diferenciais_competitivos'][:3])
        
        # Filtra conceitos v√°lidos (n√£o vazios, n√£o gen√©ricos)
        valid_concepts = []
        for concept in concepts:
            if (concept and 
                len(concept) > 20 and 
                'customizado para' not in concept.lower() and
                'baseado em' not in concept.lower()):
                valid_concepts.append(concept)
        
        return valid_concepts[:10]  # M√°ximo 10 conceitos

    def _consolidate_gigantic_analysis(
        self,
        data: Dict[str, Any],
        research_data: Dict[str, Any],
        ai_analysis: Dict[str, Any],
        advanced_components: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Consolida an√°lise GIGANTE final"""

        # Obt√©m apenas componentes bem-sucedidos
        successful_components = self.dependency_manager.get_successful_components()

        consolidated_analysis = {
            "projeto_dados": data,
            "pesquisa_web_massiva": {
                "estatisticas": {
                    "total_queries": research_data.get('total_queries', 0),
                    "total_resultados": research_data.get('total_results', 0),
                    "fontes_unicas": research_data.get('unique_sources', 0),
                    "total_conteudo": research_data.get('total_content_length', 0),
                    "extra√ß√µes_bem_sucedidas": research_data.get('successful_extractions', 0),
                    "qualidade_media": research_data.get('quality_metrics', {}).get('avg_quality_score', 0)
                },
                "fontes": research_data.get('sources', [])
            },
            **ai_analysis,  # Inclui avatar, escopo, etc.
            **successful_components,  # Inclui apenas componentes bem-sucedidos
            "consolidacao_timestamp": datetime.now().isoformat(),
            "component_status": {
                "successful": list(successful_components.keys()),
                "failed": list(self.dependency_manager.get_failure_report().keys()),
                "total_attempted": len(self.dependency_manager.component_status)
            }
        }

        return consolidated_analysis

    def _calculate_final_quality_score(self, final_analysis: Dict[str, Any]) -> float:
        """Calcula score final de qualidade"""

        score = 0.0
        max_score = 100.0

        # Verifica pesquisa massiva (30 pontos)
        pesquisa = final_analysis.get('pesquisa_web_massiva', {})
        estatisticas = pesquisa.get('estatisticas', {})
        
        if estatisticas.get('fontes_unicas', 0) >= 3:
            score += 15
        if estatisticas.get('total_conteudo', 0) >= 5000:
            score += 15

        # Verifica an√°lise IA (40 pontos)
        if final_analysis.get('avatar_ultra_detalhado'):
            score += 15
        if final_analysis.get('insights_exclusivos') and len(final_analysis['insights_exclusivos']) >= 5:
            score += 15
        if final_analysis.get('analise_concorrencia_detalhada'):
            score += 10

        # Verifica componentes avan√ßados (30 pontos)
        advanced_components = ['drivers_mentais_customizados', 'provas_visuais_sugeridas', 
                              'sistema_anti_objecao', 'pre_pitch_invisivel', 'predicoes_futuro_completas']
        
        successful_advanced = sum(1 for comp in advanced_components if comp in final_analysis)
        score += (successful_advanced / len(advanced_components)) * 30

        # Bonus por qualidade da pesquisa
        avg_quality = estatisticas.get('qualidade_media', 0)
        if avg_quality >= 80:
            score += 5
        elif avg_quality >= 60:
            score += 3

        return min(score, max_score)

    def _generate_intelligent_queries(self, data: Dict[str, Any]) -> List[str]:
        """Gera queries inteligentes para pesquisa"""

        segmento = data.get('segmento', '')
        produto = data.get('produto', '')
        publico = data.get('publico', '')

        base_queries = []

        # Queries principais
        if produto:
            base_queries.extend([
                f"mercado {segmento} {produto} Brasil 2024 dados estat√≠sticas",
                f"an√°lise competitiva {segmento} {produto} oportunidades",
                f"tend√™ncias {segmento} {produto} crescimento futuro"
            ])
        else:
            base_queries.extend([
                f"mercado {segmento} Brasil 2024 dados estat√≠sticas crescimento",
                f"an√°lise competitiva {segmento} principais empresas",
                f"tend√™ncias {segmento} oportunidades investimento"
            ])

        # Queries espec√≠ficas por p√∫blico
        if publico:
            base_queries.extend([
                f"comportamento consumidor {publico} {segmento} pesquisa",
                f"perfil demogr√°fico {publico} Brasil dados"
            ])

        # Queries de intelig√™ncia de mercado
        base_queries.extend([
            f"startups {segmento} investimento venture capital Brasil",
            f"cases sucesso empresas {segmento} brasileiras",
            f"desafios principais {segmento} solu√ß√µes mercado"
        ])

        return base_queries[:8]  # M√°ximo 8 queries para otimiza√ß√£o

    def _validate_input_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida dados de entrada - FALHA SE INSUFICIENTE"""

        required_fields = ['segmento']
        missing_fields = [field for field in required_fields if not data.get(field)]

        if missing_fields:
            return {
                'valid': False,
                'message': f"Campos obrigat√≥rios ausentes: {', '.join(missing_fields)}"
            }

        # Valida qualidade dos dados
        segmento = data.get('segmento', '').strip()
        if len(segmento) < 3:
            return {
                'valid': False,
                'message': "Segmento deve ter pelo menos 3 caracteres"
            }

        return {'valid': True, 'message': 'Dados v√°lidos'}

    def _generate_basic_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """REMOVIDO - Sistema n√£o aceita fallbacks simulados"""
        
        raise Exception("FALLBACKS SIMULADOS REMOVIDOS: Configure todas as APIs para an√°lise real.")

# Inst√¢ncia global
ultra_detailed_analysis_engine = UltraDetailedAnalysisEngine()